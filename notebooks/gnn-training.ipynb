{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12673467,"sourceType":"datasetVersion","datasetId":8009026}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-geometric\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T20:35:29.585743Z","iopub.execute_input":"2025-08-04T20:35:29.586396Z","iopub.status.idle":"2025-08-04T20:35:34.651988Z","shell.execute_reply.started":"2025-08-04T20:35:29.586366Z","shell.execute_reply":"2025-08-04T20:35:34.651270Z"}},"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.12.13)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.5.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\nfrom torch_geometric.data import Data, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=== GNN LINK PREDICTION WITH ALL FEATURES ===\\n\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# --- 1. LOAD ALL FEATURES ---\nprint(\"\\n=== LOADING FEATURES ===\")\n\n# Load embeddings\nembeddings_df = pd.read_pickle('/kaggle/input/features-ready/embeddings.pkl')\nprint(f\"Loaded embeddings: {embeddings_df.shape}\")\n\n# Load PCA features  \npca_features_df = pd.read_pickle('/kaggle/input/features-ready/node_features_pca.pkl')\nprint(f\"Loaded PCA features: {pca_features_df.shape}\")\n\n# Load graph features\ngraph_features_df = pd.read_pickle('/kaggle/input/features-ready/simple_graph_features.pkl')\nprint(f\"Loaded graph features: {graph_features_df.shape}\")\n\n# Ensure all have same nodes\ncommon_nodes = set(embeddings_df.index) & set(pca_features_df.index) & set(graph_features_df.index)\nprint(f\"Common nodes: {len(common_nodes)}\")\n\n# Combine all features\nprint(\"Combining all features...\")\nall_node_features = []\nnode_list = sorted(list(common_nodes))\n\nfor node in node_list:\n    embedding = embeddings_df.loc[node].values          # 64 dims\n    pca_features = pca_features_df.loc[node].values     # 256 dims  \n    graph_features = graph_features_df.loc[node].values # 10 dims\n    \n    combined = np.concatenate([embedding, pca_features, graph_features])\n    all_node_features.append(combined)\n\n# Convert to tensor\nnode_features_tensor = torch.FloatTensor(all_node_features)\nprint(f\"Combined features shape: {node_features_tensor.shape}\")\n\n# Create node mapping\nnode_to_idx = {node: idx for idx, node in enumerate(node_list)}\nprint(f\"Node mapping created for {len(node_to_idx)} nodes\")\n\n# --- 2. LOAD EDGES AND CREATE GRAPH ---\nprint(\"\\n=== LOADING EDGES ===\")\n\ntrain_edges_df = pd.read_csv('/kaggle/input/features-ready/train_positives.csv')\ntest_edges_df = pd.read_csv('/kaggle/input/features-ready/test_positives (1).csv')\n\nprint(f\"Train edges: {len(train_edges_df)}\")\nprint(f\"Test edges: {len(test_edges_df)}\")\n\n# Convert edges to indices\ndef edges_to_indices(edges_df, node_mapping):\n    valid_edges = []\n    for _, row in edges_df.iterrows():\n        if row['node_1'] in node_mapping and row['node_2'] in node_mapping:\n            idx1 = node_mapping[row['node_1']]\n            idx2 = node_mapping[row['node_2']]\n            valid_edges.append([idx1, idx2])\n    return torch.LongTensor(valid_edges).t()\n\ntrain_edge_index = edges_to_indices(train_edges_df, node_to_idx)\ntest_edge_index = edges_to_indices(test_edges_df, node_to_idx)\n\nprint(f\"Train edge tensor shape: {train_edge_index.shape}\")\nprint(f\"Test edge tensor shape: {test_edge_index.shape}\")\n\n# --- 3. GENERATE NEGATIVE EDGES ---\nprint(\"\\n=== GENERATING NEGATIVE EDGES ===\")\n\ndef generate_negative_edges(num_nodes, positive_edges, num_negatives):\n    \"\"\"Generate negative edges that don't exist in positive edges\"\"\"\n    positive_set = set()\n    for i in range(positive_edges.shape[1]):\n        edge = tuple(sorted([positive_edges[0, i].item(), positive_edges[1, i].item()]))\n        positive_set.add(edge)\n    \n    negative_edges = []\n    while len(negative_edges) < num_negatives:\n        node1 = random.randint(0, num_nodes - 1)\n        node2 = random.randint(0, num_nodes - 1)\n        \n        if node1 != node2:\n            edge = tuple(sorted([node1, node2]))\n            if edge not in positive_set:\n                negative_edges.append([node1, node2])\n                positive_set.add(edge)  # Avoid duplicates\n    \n    return torch.LongTensor(negative_edges).t()\n\n# Generate negative edges\ntrain_neg_edge_index = generate_negative_edges(\n    len(node_list), train_edge_index, train_edge_index.shape[1]\n)\ntest_neg_edge_index = generate_negative_edges(\n    len(node_list), \n    torch.cat([train_edge_index, test_edge_index], dim=1),\n    test_edge_index.shape[1]\n)\n\nprint(f\"Train negative edges: {train_neg_edge_index.shape}\")\nprint(f\"Test negative edges: {test_neg_edge_index.shape}\")\n\n# --- 4. DEFINE GNN MODEL ---\nprint(\"\\n=== DEFINING GNN MODEL ===\")\n\nclass LinkPredictionGNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128, num_layers=2):\n        super(LinkPredictionGNN, self).__init__()\n        \n        # GNN layers\n        self.convs = nn.ModuleList()\n        self.convs.append(GCNConv(input_dim, hidden_dim))\n        \n        for _ in range(num_layers - 1):\n            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n        \n        # Link prediction head\n        self.link_predictor = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim // 2, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x, edge_index, edge_pairs):\n        # GNN forward pass\n        for i, conv in enumerate(self.convs):\n            x = conv(x, edge_index)\n            if i < len(self.convs) - 1:\n                x = F.relu(x)\n                x = F.dropout(x, training=self.training)\n        \n        # Get embeddings for edge pairs\n        row, col = edge_pairs\n        edge_embeddings = torch.cat([x[row], x[col]], dim=1)\n        \n        # Predict link probability\n        return self.link_predictor(edge_embeddings)\n\n# Initialize model\ninput_dim = node_features_tensor.shape[1]\nmodel = LinkPredictionGNN(input_dim, hidden_dim=128, num_layers=2)\nmodel = model.to(device)\n\nprint(f\"Model created with input dim: {input_dim}\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# --- 5. TRAINING PREPARATION ---\nprint(\"\\n=== PREPARING TRAINING ===\")\n\n# Move data to device\nnode_features_tensor = node_features_tensor.to(device)\ntrain_edge_index = train_edge_index.to(device)\n\n# Prepare training data\ntrain_pos_pairs = train_edge_index\ntrain_neg_pairs = train_neg_edge_index.to(device)\n\n# Combine positive and negative pairs\ntrain_pairs = torch.cat([train_pos_pairs, train_neg_pairs], dim=1)\ntrain_labels = torch.cat([\n    torch.ones(train_pos_pairs.shape[1]),\n    torch.zeros(train_neg_pairs.shape[1])\n]).to(device)\n\nprint(f\"Training pairs: {train_pairs.shape[1]}\")\nprint(f\"Positive: {train_pos_pairs.shape[1]}, Negative: {train_neg_pairs.shape[1]}\")\n\n# --- 6. TRAINING LOOP ---\nprint(\"\\n=== TRAINING MODEL ===\")\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\ncriterion = nn.BCELoss()\n\nnum_epochs = 400\nbest_loss = float('inf')\n\nmodel.train()\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    \n    # Forward pass\n    predictions = model(node_features_tensor, train_edge_index, train_pairs).squeeze()\n    \n    # Compute loss\n    loss = criterion(predictions, train_labels)\n    \n    # Backward pass\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 20 == 0:\n        print(f\"Epoch {epoch:3d}, Loss: {loss.item():.4f}\")\n    \n    if loss.item() < best_loss:\n        best_loss = loss.item()\n        torch.save(model.state_dict(), '/kaggle/working/best_gnn_model.pth')\n\nprint(f\"Training completed. Best loss: {best_loss:.4f}\")\n\n# --- 7. TESTING ---\nprint(\"\\n=== TESTING MODEL ===\")\n\n# Load best model\nmodel.load_state_dict(torch.load('/kaggle/working/best_gnn_model.pth'))\nmodel.eval()\n\n# Prepare test data\ntest_pos_pairs = test_edge_index.to(device)\ntest_neg_pairs = test_neg_edge_index.to(device)\n\ntest_pairs = torch.cat([test_pos_pairs, test_neg_pairs], dim=1)\ntest_labels = torch.cat([\n    torch.ones(test_pos_pairs.shape[1]),\n    torch.zeros(test_neg_pairs.shape[1])\n]).cpu().numpy()\n\nprint(f\"Test pairs: {test_pairs.shape[1]}\")\nprint(f\"Positive: {test_pos_pairs.shape[1]}, Negative: {test_neg_pairs.shape[1]}\")\n\n# Get predictions\nwith torch.no_grad():\n    test_predictions = model(node_features_tensor, train_edge_index, test_pairs).squeeze().cpu().numpy()\n\n# Convert probabilities to binary predictions\ntest_pred_binary = (test_predictions > 0.5).astype(int)\n\n# Compute metrics\naccuracy = accuracy_score(test_labels, test_pred_binary)\nf1 = f1_score(test_labels, test_pred_binary)\nprecision = precision_score(test_labels, test_pred_binary)\nrecall = recall_score(test_labels, test_pred_binary)\nauc = roc_auc_score(test_labels, test_predictions)\n\nprint(f\"\\n=== TEST RESULTS ===\")\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"AUC:       {auc:.4f}\")\n\n# --- 8. ADDITIONAL SIMILARITY-BASED ANALYSIS ---\nprint(\"\\n=== SIMILARITY-BASED ANALYSIS ===\")\n\n# Get final node embeddings from GNN\nwith torch.no_grad():\n    final_embeddings = model.convs[0](node_features_tensor, train_edge_index)\n    for i in range(1, len(model.convs)):\n        final_embeddings = model.convs[i](final_embeddings, train_edge_index)\n\nfinal_embeddings = final_embeddings.cpu().numpy()\n\ndef compute_similarity_features(node1_idx, node2_idx, embeddings):\n    \"\"\"Compute various similarity features between two nodes\"\"\"\n    emb1 = embeddings[node1_idx].reshape(1, -1)\n    emb2 = embeddings[node2_idx].reshape(1, -1)\n    \n    # Cosine similarity\n    cos_sim = cosine_similarity(emb1, emb2)[0, 0]\n    \n    # Euclidean distance\n    euclidean_dist = np.linalg.norm(emb1 - emb2)\n    \n    # Absolute difference (L1 norm)\n    l1_dist = np.sum(np.abs(emb1 - emb2))\n    \n    # Dot product\n    dot_product = np.dot(emb1.flatten(), emb2.flatten())\n    \n    return {\n        'cosine_similarity': cos_sim,\n        'euclidean_distance': euclidean_dist,\n        'l1_distance': l1_dist,\n        'dot_product': dot_product\n    }\n\n# Compute similarity features for test pairs\nprint(\"Computing similarity features for test pairs...\")\nsimilarity_features = []\nfor i in range(test_pairs.shape[1]):\n    node1_idx = test_pairs[0, i].item()\n    node2_idx = test_pairs[1, i].item()\n    sim_feats = compute_similarity_features(node1_idx, node2_idx, final_embeddings)\n    similarity_features.append(sim_feats)\n\n# Convert to DataFrame for analysis\nsim_df = pd.DataFrame(similarity_features)\nsim_df['label'] = test_labels\nsim_df['gnn_prediction'] = test_predictions\n\nprint(\"\\nSimilarity feature statistics by label:\")\nprint(sim_df.groupby('label').agg({\n    'cosine_similarity': ['mean', 'std'],\n    'euclidean_distance': ['mean', 'std'],\n    'l1_distance': ['mean', 'std'],\n    'dot_product': ['mean', 'std']\n}))\n\n# Simple cosine similarity baseline\ncos_sim_threshold = 0.5\ncos_sim_predictions = (sim_df['cosine_similarity'] > cos_sim_threshold).astype(int)\ncos_sim_f1 = f1_score(test_labels, cos_sim_predictions)\n\nprint(f\"\\nBaseline cosine similarity F1 (threshold={cos_sim_threshold}): {cos_sim_f1:.4f}\")\nprint(f\"GNN F1 score: {f1:.4f}\")\nprint(f\"Improvement: {f1 - cos_sim_f1:.4f}\")\n\n# --- 9. SAVE RESULTS ---\nprint(\"\\n=== SAVING RESULTS ===\")\n\nresults = {\n    'model_state_dict': model.state_dict(),\n    'test_metrics': {\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'precision': precision,\n        'recall': recall,\n        'auc': auc\n    },\n    'similarity_analysis': sim_df,\n    'node_to_idx_mapping': node_to_idx,\n    'final_embeddings': final_embeddings,\n    'model_config': {\n        'input_dim': input_dim,\n        'hidden_dim': 128,\n        'num_layers': 2\n    }\n}\n\nwith open('/kaggle/working/gnn_results.pkl', 'wb') as f:\n    pickle.dump(results, f)\n\nprint(\"✅ Results saved to '/kaggle/working/gnn_results.pkl'\")\n\nprint(f\"\\n🎯 FINAL SUMMARY:\")\nprint(f\"✅ Trained GNN on {input_dim}-dimensional features\")\nprint(f\"✅ Used {len(node_list)} nodes, {train_pairs.shape[1]} training pairs\")\nprint(f\"✅ Test F1 Score: {f1:.4f}\")\nprint(f\"✅ Test Accuracy: {accuracy:.4f}\")\nprint(f\"✅ Model and results saved for future use\")\n\n# --- 10. PREDICTION FUNCTION ---\nprint(\"\\n=== CREATING PREDICTION FUNCTION ===\")\n\ndef predict_link(node1_id, node2_id, model, node_features, edge_index, node_mapping):\n    \"\"\"Predict if two nodes should be linked\"\"\"\n    if node1_id not in node_mapping or node2_id not in node_mapping:\n        return {\"error\": \"Node not found in mapping\"}\n    \n    model.eval()\n    with torch.no_grad():\n        idx1 = node_mapping[node1_id]\n        idx2 = node_mapping[node2_id]\n        \n        test_pair = torch.LongTensor([[idx1], [idx2]]).to(device)\n        prediction = model(node_features, edge_index, test_pair).item()\n        \n        return {\n            'node1': node1_id,\n            'node2': node2_id,\n            'probability': prediction,\n            'prediction': 'LINK' if prediction > 0.5 else 'NO LINK'\n        }\n\n# Test the prediction function\nprint(\"Testing prediction function...\")\ntest_prediction = predict_link(1777, 19409, model, node_features_tensor, train_edge_index, node_to_idx)\nprint(f\"Sample prediction: {test_prediction}\")\n\nprint(\"\\n🚀 GNN Link Prediction Pipeline Complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-04T20:41:03.826909Z","iopub.execute_input":"2025-08-04T20:41:03.827231Z","iopub.status.idle":"2025-08-04T20:41:33.615469Z","shell.execute_reply.started":"2025-08-04T20:41:03.827209Z","shell.execute_reply":"2025-08-04T20:41:33.614675Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"=== GNN LINK PREDICTION WITH ALL FEATURES ===\n\nUsing device: cuda\n\n=== LOADING FEATURES ===\nLoaded embeddings: (28281, 64)\nLoaded PCA features: (28281, 256)\nLoaded graph features: (28281, 10)\nCommon nodes: 28281\nCombining all features...\nCombined features shape: torch.Size([28281, 330])\nNode mapping created for 28281 nodes\n\n=== LOADING EDGES ===\nTrain edges: 74201\nTest edges: 18551\nTrain edge tensor shape: torch.Size([2, 74201])\nTest edge tensor shape: torch.Size([2, 18551])\n\n=== GENERATING NEGATIVE EDGES ===\nTrain negative edges: torch.Size([2, 74201])\nTest negative edges: torch.Size([2, 18551])\n\n=== DEFINING GNN MODEL ===\nModel created with input dim: 330\nModel parameters: 100,097\n\n=== PREPARING TRAINING ===\nTraining pairs: 148402\nPositive: 74201, Negative: 74201\n\n=== TRAINING MODEL ===\nEpoch   0, Loss: 0.6988\nEpoch  20, Loss: 0.5439\nEpoch  40, Loss: 0.4365\nEpoch  60, Loss: 0.3396\nEpoch  80, Loss: 0.2991\nEpoch 100, Loss: 0.2696\nEpoch 120, Loss: 0.2486\nEpoch 140, Loss: 0.2230\nEpoch 160, Loss: 0.2045\nEpoch 180, Loss: 0.1868\nEpoch 200, Loss: 0.1725\nEpoch 220, Loss: 0.1606\nEpoch 240, Loss: 0.1495\nEpoch 260, Loss: 0.1388\nEpoch 280, Loss: 0.1313\nEpoch 300, Loss: 0.1237\nEpoch 320, Loss: 0.1165\nEpoch 340, Loss: 0.1093\nEpoch 360, Loss: 0.1046\nEpoch 380, Loss: 0.0989\nTraining completed. Best loss: 0.0947\n\n=== TESTING MODEL ===\nTest pairs: 37102\nPositive: 18551, Negative: 18551\n\n=== TEST RESULTS ===\nAccuracy:  0.8214\nF1 Score:  0.8031\nPrecision: 0.8945\nRecall:    0.7287\nAUC:       0.8786\n\n=== SIMILARITY-BASED ANALYSIS ===\nComputing similarity features for test pairs...\n\nSimilarity feature statistics by label:\n      cosine_similarity           euclidean_distance            l1_distance  \\\n                   mean       std               mean        std        mean   \nlabel                                                                         \n0.0            0.558204  0.539644          76.405899  58.530117  722.329834   \n1.0            0.794102  0.475749          90.916580  92.225716  861.600464   \n\n                    dot_product                \n              std          mean           std  \nlabel                                          \n0.0    561.315002   4820.098145   7268.706055  \n1.0    879.449585  17192.287109  26480.796005  \n\nBaseline cosine similarity F1 (threshold=0.5): 0.6823\nGNN F1 score: 0.8031\nImprovement: 0.1208\n\n=== SAVING RESULTS ===\n✅ Results saved to '/kaggle/working/gnn_results.pkl'\n\n🎯 FINAL SUMMARY:\n✅ Trained GNN on 330-dimensional features\n✅ Used 28281 nodes, 148402 training pairs\n✅ Test F1 Score: 0.8031\n✅ Test Accuracy: 0.8214\n✅ Model and results saved for future use\n\n=== CREATING PREDICTION FUNCTION ===\nTesting prediction function...\nSample prediction: {'node1': 1777, 'node2': 19409, 'probability': 0.9953808784484863, 'prediction': 'LINK'}\n\n🚀 GNN Link Prediction Pipeline Complete!\n","output_type":"stream"}],"execution_count":9}]}